# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16wWtsr4mBkQu-haAvXfrEokVBePrVqxV
"""



"""**LDA - Topic Modelling**

---




"""

#Paramters():
#inputs: Document term matrix, (bow or tfidf)
#no_of_topics we want
#no of epocs we want

"""**What it does?**

---


It learns the topic mix in each word and the word mix in each topic
The output is the the distribution of words in each topic seperated

Lets say we the model A 100 topics.
Set the number of topics as (K=2).
Then the model randomly assigns the words to any of the topic.
Then it goes through each word and checks how many times it occurs in each TOPIC and reassigns unit our number of epocs are reached.

1)Checks how often topic appears in doc.
2)Checks how often words occur in topic
"""

import gensim

import nltk
import wikipedia as w

nltk.download()

dora=w.page("Doraemon")
kgp=w.page("IIT Kharagpur")
baski=w.page("Basketball")
covid=w.page("Coronavirus disease 2019")

corpus=[dora.content,kgp.content,data.content,covid.content]
len(corpus)

import re

from bs4 import BeautifulSoup

from nltk.stem import LancasterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
  

lemma=WordNetLemmatizer()

def preprocess(doc):
  doc=re.sub(r'\W', ' ',str(doc))
  doc = doc.lower()                 # Converting to lowercase
  cleanr = re.compile('<.*?>')
  doc = re.sub(cleanr, ' ',str(doc))        #Removing HTML tags
  doc = re.sub(r'[?|!|\'|"|#]',r'',str(doc))
  doc = re.sub(r'[.|,|)|(|\|/]',r' ',str(doc))
  doc=re.sub(r'\s+', ' ',str(doc),flags=re.I)
  doc=re.sub(r'^b\s+', ' ',str(doc))
  doc = re.sub(r'\[[0-9]*\]', ' ', doc)
  doc = re.sub(r'\s+', ' ',doc)
  # Removing special characters and digits
  doc = re.sub('[^a-zA-Z]', ' ', doc )
  doc = re.sub(r'\s+', ' ', doc)
  doc_list = nltk.sent_tokenize(doc)
  stopwords = nltk.corpus.stopwords.words('english')
  #Lemmatization
  tokens=doc.split()
  tokens=[lemma.lemmatize(word) for word in tokens]
  tokens=[word for word in tokens if word not in stopwords]
  return tokens

processed_data=[]
for text in corpus:
    tokens=preprocess(text)
    processed_data.append(tokens)

processed_data[0:10]

from gensim import corpora

input_dict=corpora.Dictionary(processed_data)
input_corpus=[input_dict.doc2bow(token,allow_update=True) for token in processed_data]

input_corpus[0:10]

lda=gensim.models.ldamodel.LdaModel(input_corpus,num_topics=4,id2word=input_dict,passes=50)

topics=lda.print_topics(num_words=20)
topics

for t in topics:
    print(t)

